{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # pretrained YOLO11n model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 18 persons, 25.4ms\n",
      "Speed: 1.5ms preprocess, 25.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/predict9\u001b[0m\n",
      "1 label saved to runs/predict9/labels\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.7255, 0.7132, 0.7050, 0.7040, 0.6979, 0.6739, 0.6467, 0.5905, 0.5539, 0.5418, 0.5375, 0.4960, 0.4824, 0.4258, 0.3273, 0.3265, 0.2951, 0.2586])\n",
      "data: tensor([[2.1182e+02, 2.8149e+02, 2.6167e+02, 3.4522e+02, 7.2551e-01, 0.0000e+00],\n",
      "        [7.3428e+02, 5.1832e+02, 7.8120e+02, 5.9885e+02, 7.1318e-01, 0.0000e+00],\n",
      "        [8.5425e+02, 2.1593e+02, 8.9102e+02, 2.7797e+02, 7.0501e-01, 0.0000e+00],\n",
      "        [3.8320e+02, 2.6412e+02, 4.2435e+02, 3.2565e+02, 7.0398e-01, 0.0000e+00],\n",
      "        [1.1122e+03, 3.5492e+02, 1.1593e+03, 4.2393e+02, 6.9786e-01, 0.0000e+00],\n",
      "        [9.3750e+02, 3.0359e+02, 9.7652e+02, 3.6851e+02, 6.7392e-01, 0.0000e+00],\n",
      "        [8.2709e+02, 2.8291e+02, 8.5895e+02, 3.4326e+02, 6.4674e-01, 0.0000e+00],\n",
      "        [4.7239e+02, 4.1317e+02, 5.2589e+02, 4.8881e+02, 5.9049e-01, 0.0000e+00],\n",
      "        [6.4070e+02, 2.5024e+02, 6.6596e+02, 3.0599e+02, 5.5392e-01, 0.0000e+00],\n",
      "        [7.7307e+02, 2.9951e+02, 8.1654e+02, 3.6157e+02, 5.4179e-01, 0.0000e+00],\n",
      "        [7.3826e+02, 2.9650e+02, 7.6055e+02, 3.4490e+02, 5.3750e-01, 0.0000e+00],\n",
      "        [1.1004e+03, 2.3198e+02, 1.1388e+03, 3.0003e+02, 4.9603e-01, 0.0000e+00],\n",
      "        [5.5489e+02, 2.4708e+02, 5.8411e+02, 3.0763e+02, 4.8244e-01, 0.0000e+00],\n",
      "        [7.3607e+02, 2.9569e+02, 7.6255e+02, 3.5977e+02, 4.2578e-01, 0.0000e+00],\n",
      "        [1.1455e+03, 1.7892e+02, 1.1706e+03, 2.3661e+02, 3.2734e-01, 0.0000e+00],\n",
      "        [5.7268e+02, 2.1689e+02, 5.9662e+02, 2.7727e+02, 3.2647e-01, 0.0000e+00],\n",
      "        [1.1112e+03, 1.9137e+02, 1.1401e+03, 2.4336e+02, 2.9508e-01, 0.0000e+00],\n",
      "        [5.6355e+02, 2.1663e+02, 6.0036e+02, 3.0651e+02, 2.5856e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (720, 1280)\n",
      "shape: torch.Size([18, 6])\n",
      "xywh: tensor([[ 236.7418,  313.3547,   49.8521,   63.7239],\n",
      "        [ 757.7419,  558.5876,   46.9191,   80.5273],\n",
      "        [ 872.6347,  246.9527,   36.7778,   62.0439],\n",
      "        [ 403.7704,  294.8849,   41.1500,   61.5278],\n",
      "        [1135.7476,  389.4207,   47.1226,   69.0101],\n",
      "        [ 957.0099,  336.0472,   39.0139,   64.9202],\n",
      "        [ 843.0191,  313.0869,   31.8662,   60.3542],\n",
      "        [ 499.1384,  450.9897,   53.5010,   75.6444],\n",
      "        [ 653.3277,  278.1166,   25.2644,   55.7440],\n",
      "        [ 794.8032,  330.5402,   43.4736,   62.0613],\n",
      "        [ 749.4067,  320.7001,   22.2957,   48.3929],\n",
      "        [1119.6265,  266.0035,   38.4287,   68.0469],\n",
      "        [ 569.4986,  277.3521,   29.2239,   60.5503],\n",
      "        [ 749.3093,  327.7325,   26.4830,   64.0836],\n",
      "        [1158.0518,  207.7632,   25.1604,   57.6862],\n",
      "        [ 584.6490,  247.0814,   23.9368,   60.3822],\n",
      "        [1125.6520,  217.3618,   28.8474,   51.9900],\n",
      "        [ 581.9536,  261.5679,   36.8049,   89.8828]])\n",
      "xywhn: tensor([[0.1850, 0.4352, 0.0389, 0.0885],\n",
      "        [0.5920, 0.7758, 0.0367, 0.1118],\n",
      "        [0.6817, 0.3430, 0.0287, 0.0862],\n",
      "        [0.3154, 0.4096, 0.0321, 0.0855],\n",
      "        [0.8873, 0.5409, 0.0368, 0.0958],\n",
      "        [0.7477, 0.4667, 0.0305, 0.0902],\n",
      "        [0.6586, 0.4348, 0.0249, 0.0838],\n",
      "        [0.3900, 0.6264, 0.0418, 0.1051],\n",
      "        [0.5104, 0.3863, 0.0197, 0.0774],\n",
      "        [0.6209, 0.4591, 0.0340, 0.0862],\n",
      "        [0.5855, 0.4454, 0.0174, 0.0672],\n",
      "        [0.8747, 0.3694, 0.0300, 0.0945],\n",
      "        [0.4449, 0.3852, 0.0228, 0.0841],\n",
      "        [0.5854, 0.4552, 0.0207, 0.0890],\n",
      "        [0.9047, 0.2886, 0.0197, 0.0801],\n",
      "        [0.4568, 0.3432, 0.0187, 0.0839],\n",
      "        [0.8794, 0.3019, 0.0225, 0.0722],\n",
      "        [0.4547, 0.3633, 0.0288, 0.1248]])\n",
      "xyxy: tensor([[ 211.8158,  281.4928,  261.6678,  345.2167],\n",
      "        [ 734.2823,  518.3240,  781.2014,  598.8513],\n",
      "        [ 854.2458,  215.9307,  891.0236,  277.9747],\n",
      "        [ 383.1954,  264.1210,  424.3454,  325.6489],\n",
      "        [1112.1863,  354.9156, 1159.3088,  423.9258],\n",
      "        [ 937.5029,  303.5871,  976.5168,  368.5073],\n",
      "        [ 827.0860,  282.9098,  858.9522,  343.2640],\n",
      "        [ 472.3879,  413.1675,  525.8889,  488.8119],\n",
      "        [ 640.6955,  250.2446,  665.9599,  305.9886],\n",
      "        [ 773.0663,  299.5096,  816.5400,  361.5709],\n",
      "        [ 738.2589,  296.5036,  760.5546,  344.8966],\n",
      "        [1100.4121,  231.9801, 1138.8408,  300.0269],\n",
      "        [ 554.8867,  247.0769,  584.1105,  307.6272],\n",
      "        [ 736.0678,  295.6907,  762.5508,  359.7743],\n",
      "        [1145.4716,  178.9201, 1170.6320,  236.6063],\n",
      "        [ 572.6807,  216.8903,  596.6174,  277.2725],\n",
      "        [1111.2283,  191.3668, 1140.0757,  243.3568],\n",
      "        [ 563.5511,  216.6266,  600.3561,  306.5093]])\n",
      "xyxyn: tensor([[0.1655, 0.3910, 0.2044, 0.4795],\n",
      "        [0.5737, 0.7199, 0.6103, 0.8317],\n",
      "        [0.6674, 0.2999, 0.6961, 0.3861],\n",
      "        [0.2994, 0.3668, 0.3315, 0.4523],\n",
      "        [0.8689, 0.4929, 0.9057, 0.5888],\n",
      "        [0.7324, 0.4216, 0.7629, 0.5118],\n",
      "        [0.6462, 0.3929, 0.6711, 0.4768],\n",
      "        [0.3691, 0.5738, 0.4109, 0.6789],\n",
      "        [0.5005, 0.3476, 0.5203, 0.4250],\n",
      "        [0.6040, 0.4160, 0.6379, 0.5022],\n",
      "        [0.5768, 0.4118, 0.5942, 0.4790],\n",
      "        [0.8597, 0.3222, 0.8897, 0.4167],\n",
      "        [0.4335, 0.3432, 0.4563, 0.4273],\n",
      "        [0.5751, 0.4107, 0.5957, 0.4997],\n",
      "        [0.8949, 0.2485, 0.9146, 0.3286],\n",
      "        [0.4474, 0.3012, 0.4661, 0.3851],\n",
      "        [0.8681, 0.2658, 0.8907, 0.3380],\n",
      "        [0.4403, 0.3009, 0.4690, 0.4257]])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicheraghian/Documents/Github/AI_Football_Analyzer/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "# Open the video file\n",
    "video_path = \"../data/raw/videos/CityUtdR.mp4\"  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Read and display frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # apply object detection\n",
    "    results = model.predict(frame, save=True, project=\"runs\")  # saves to runs/detect/exp\n",
    "\n",
    "    for r in results:\n",
    "        print(r.boxes)  # print the Boxes object containing the detection bounding boxes\n",
    "\n",
    "    sys.exit(0)\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Football', (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "        # display the frame\n",
    "        cv2.imshow(\"Video Frame\", frame)\n",
    "\n",
    "        # press 'q' to exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if not ret:  # Break the loop if no more frames\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"Video Frame\", frame)  # Display frame\n",
    "    \n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
